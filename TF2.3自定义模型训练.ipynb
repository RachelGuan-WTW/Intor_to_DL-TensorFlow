{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 介绍TensorFlow自定义训练模型\n",
    "可以帮助我们理解深度学习，但是还是Keras模型封装使用方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导tf.GradentTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GradientTape**是eager模式下计算梯度用的\n",
    "\n",
    "- **watch(tensor)**\n",
    "\n",
    "  作用：确保某个tensor被tape追踪 \n",
    "\n",
    "  参数:tensor: 一个Tensor或者一个Tensor列表\n",
    "\n",
    "- **gradient(target, sources)**\n",
    "\n",
    "  作用：根据tape上面的上下文来计算某个或者某些tensor的梯度参数\n",
    "\n",
    "  target: 被微分的Tensor或者Tensor列表，你可以理解为经过某个函数之后的值\n",
    "\n",
    "  sources: Tensors 或者Variables列表（当然可以只有一个值）. 你可以理解为函数的某个变量\n",
    "\n",
    "  返回:\n",
    "  \n",
    "  一个列表表示各个变量的梯度值，和source中的变量列表一一对应，表明这个变量的梯度。\n",
    "  \n",
    "  上面的例子中的梯度计算部分可以更直观的理解这个函数的用法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#求y=x^2在x=3处的导数\n",
    "x=tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y=x*x#把要求导的函数放在with块里面\n",
    "    \n",
    "dy_dx=g.gradient(y,x)\n",
    "\n",
    "tf.print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模型\n",
    "包括但是不限于：\n",
    "- 构建基本的模型结构（构建前向传播、定义损失函数、定义优化函数、求导、得到预测值、后向传播，更新参数）\n",
    "- 构建多层循环(epoch)、多层数据（batch）\n",
    "- 构建评估函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu') #隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)#输出层\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #下面是前向传播与后向传播的一个完整过程，详细解释\n",
    "# model = MyModel(num_classes=10)\n",
    "\n",
    "# #损失函数\n",
    "# loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# #优化器\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# #梯度\n",
    "# with tf.GradientTape() as tape:\n",
    "#     predictions = model(data)\n",
    "#     loss = loss_object(labels, predictions)\n",
    "    \n",
    "# gradients = tape.gradient(loss, model.trainable_variables) #求梯度\n",
    "\n",
    "# optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # 把计算出来的梯度更新到变量上去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述是前向传播与后向传播的一个完整过程；然后将之作为TensorFlow模型的迭代循环的一个迭代\n",
    "- 每个Batch循环，然后迭代更新\n",
    "- 每个epoch循环，然后迭代更新\n",
    "另外，还需要添加评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10分类问题\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.random((1000, 10))\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.random((200, 10))\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.random((200, 10))\n",
    "\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# 损失函数\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 准备metrics函数\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# 准备训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 准备测试数据集\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training acc over epoch: 0.0989999994635582\n",
      "Validation acc: 0.07999999821186066\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.10000000149011612\n",
      "Validation acc: 0.08500000089406967\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.10300000011920929\n",
      "Validation acc: 0.08500000089406967\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):        \n",
    "        \n",
    "        #一个batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicts = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, predicts)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))####\n",
    "\n",
    "        # 更新训练集的metrics\n",
    "        train_acc_metric(y_batch_train, predicts)     \n",
    "            \n",
    "            \n",
    "    # 在每个epoch结束时显示metrics。\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # 在每个epoch结束时重置训练指标\n",
    "    train_acc_metric.reset_states()#!!!!!!!!!!!!!!!\n",
    "\n",
    "    # 在每个epoch结束时运行一个验证集。\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_predicts = model(x_batch_val)\n",
    "        # 更新验证集merics\n",
    "        val_acc_metric(y_batch_val, val_predicts)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    print('Validation acc: %s' % (float(val_acc),))\n",
    "    val_acc_metric.reset_states()\n",
    "    \n",
    "    #显示测试集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
